\documentclass[letterpaper]{article} 
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{array}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage[version=4]{mhchem}
\usepackage{stmaryrd}
\usepackage[dvipsnames]{xcolor}
\colorlet{LightRubineRed}{RubineRed!70}
\colorlet{Mycolor1}{green!10!orange}
\definecolor{Mycolor2}{HTML}{00F9DE}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{capt-of}
\usepackage{lipsum}
\usepackage{fancyvrb}
\usepackage{tabularx}
\usepackage{listings}
\usepackage[export]{adjustbox}
\graphicspath{ {./images/} }
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{float}
\usepackage{lipsum}
\usepackage{graphicx}
\usepackage{float}
\usepackage[margin=0.7in]{geometry}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{capt-of}
\usepackage{tcolorbox}
\usepackage{lipsum}
\usepackage{graphicx}
\usepackage{float}
\usepackage{listings}
\usepackage{hyperref} 
\usepackage{xcolor} % For custom colors
\lstset{
	language=Python,                % Choose the language (e.g., Python, C, R)
	basicstyle=\ttfamily\small, % Font size and type
	keywordstyle=\color{blue},  % Keywords color
	commentstyle=\color{gray},  % Comments color
	stringstyle=\color{red},    % String color
	numbers=left,               % Line numbers
	numberstyle=\tiny\color{gray}, % Line number style
	stepnumber=1,               % Numbering step
	breaklines=true,            % Auto line break
	backgroundcolor=\color{black!5}, % Light gray background
	frame=single,               % Frame around the code
}
\usepackage{float}
\usepackage[]{amsthm} %lets us use \begin{proof}
	\usepackage[]{amssymb} %gives us the character \varnothing
	
	\title{Homework 2, STAT 5398}
	\author{Zongyi Liu}
	\date{Wed, Oct 1, 2025}
	\begin{document}
		\maketitle
		
		\section{Preparation}
		\subsection{Data and models}
		
		In this assignment, we are asked to used various financial tools and models. 
		
		We are asked to use two large language models, \href{https://huggingface.co/meta-llama/Llama-3.1-8B}{Llama-3.1-8B} and \href{https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B}{DeepSeek-R1-Distill-Llama-8B}. They are provided on the Hugging Face. 
		
		\textbf{Llama-3.1-8B} is an 8-billion-parameter large language model released by Meta in 2024. It supports multilingual text and code generation, uses Grouped-Query Attention for efficient inference, and provides a long 128k-token context window. As a relatively lightweight model in the Llama~3.1 family, it offers strong instruction-following ability while remaining practical to deploy on limited hardware\footnote{\href{https://ai.meta.com/blog/meta-llama-3-1/}{Introducing Llama 3.1: Our most capable models to date}, by Meta, July 23, 2024}. 
		
		\textbf{DeepSeek-R1-Distill-Llama-8B} is an 8-billion-parameter model obtained by distilling the larger DeepSeek-R1 into a Llama-3.1-8B backbone. The model inherits enhanced reasoning and mathematical capabilities from its teacher while remaining lightweight enough for efficient deployment. As a distilled model, it offers a strong balance between performance and computational cost, making it suitable for general reasoning, coding, and problem-solving tasks\footnote{\href{https://docs.api.nvidia.com/nim/reference/deepseek-ai-deepseek-r1-distill-llama-8b}{DeepSeek-R1-Distill-Llama-8B}, by NVIDIA API Documentation}.
		
		
		
		
		
		\subsection{Hardware configuration}
		
		I used my MacBook to implement this problem:
		
		\begin{verbatim}
     Processor      2.4 GHz Quad-Core Intel Core i5
     Graphics       Intel Iris Plus Graphics 655 1536 MB
     Memory         8 GB 2133 MHz LPDDR3
     Serial number  C02YQ0CULVDD
     macOS          Sonoma 14.6.1
		\end{verbatim}
	
	This Mac was bought in 2019 before the LLM era, it is sufficient for coding, browsing, study and office work, but may not be helpful to run large models. It can run small ML models only on CPU, but slowly. It may also be hard to train modern deep-learning models.
	
	As for graphics, it is Intel Iris Plus Graphics 655 (1536 MB shared memory); this is an integrated GPU, not a dedicated GPU. It cannot run NVIDIA-based GPU ML acceleration, and also cannot use MPS acceleration.
	
	The major bottleneck is the memory, it only has 8 GB, and today's Machine Learning models often need 12–16GB RAM just to load. We might hit “out of memory” frequently when training models.
	
	Thus we have to set up the environment like this:
	
	\begin{lstlisting}
    pip install torch  # CPU version
    pip install transformers peft datasets
	\end{lstlisting}
	 
	 I tried to implement the LLM on my laptop, but not successful, it could only be workable for mock models.  
	 
	 
		\subsection{Alternatives to solve hardware problem}
		
		There are also alternatives to solve the hardware restrictions, which is using external GPU, like Google Colab/Kaggle. Here I chose Google Colab for testing.
		
		\subsection{Low-rank adaptation}
		
		Thus we have to use the Low-Rank Adaptation (LoRA) here; the total weight matrix $\mathbf{W}$ after adaptation is the sum of the original pre-trained weight $\mathbf{W}_0$ and the update matrix $\Delta \mathbf{W}$\footnote{\cite{hu2021lora}}:
		$$
		\mathbf{W} = \mathbf{W}_0 + \Delta \mathbf{W}
		$$
		
		
		Then it involves about the low-rank factorization; the update matrix $\Delta \mathbf{W} \in \mathbb{R}^{d \times k}$ is decomposed into two low-rank matrices, $\mathbf{B}$ and $\mathbf{A}$, where $r$ is the rank ($r \ll \min(d, k)$):
		$$
		\Delta \mathbf{W} = \mathbf{B} \mathbf{A}
		$$
		The dimensions are specified as:
		$$
		\mathbf{B} \in \mathbb{R}^{d \times r} \quad \text{and} \quad \mathbf{A} \in \mathbb{R}^{r \times k}
		$$
		Thus, the final adapted weight is:
		$$
		\mathbf{W} = \mathbf{W}_0 + \mathbf{B} \mathbf{A}
		$$
		
		
		The number of trainable parameters for the update term is significantly reduced:
		\begin{itemize}
			\item {Full fine-tuning parameters:} $\mathcal{P}_{\text{full}} = d \cdot k$
			\item {LoRA trainable parameters:} $\mathcal{P}_{\text{LoRA}} = (d \cdot r) + (r \cdot k) = r (d + k)$
		\end{itemize}
	
	\section{Implementation}
	\subsection{Dow 30 dataset}
	
	Here we are given the Dow30 dataset from Hugging Face, its full name is \texttt{fingpt-forecaster-dow30-202305-202405}, it is a financial forecasting dataset designed specifically for LLM fine-tuning. It is built to train a model to generate: stock analysis, market sentiment evaluation and weekly movement predictions.
	
	It has five columns, which are:
	\begin{itemize}
		\item \texttt{prompt}: it is an instruction such as: ``You are a seasoned stock market analyst. List positive developments, risks, and provide a weekly price prediction for AAPL and MSFT..''
		
	   \item \texttt{answer}: A target response that the model should generate. For example: ``AAPL: strong iPhone demand ... Prediction: slight upward movement. MSFT: ...''
	   
	   \item \texttt{symbol}: The relevant Dow30 ticker symbols (e.g., \texttt{AAPL}, \texttt{MSFT}, \texttt{IBM}).
	   
	   \item \texttt{period}: The time range associated with the analysis  
	   (e.g., \texttt{2023-12-10 to 2023-12-17}).
	   
	   \item \texttt{label}: A categorical movement indicator like \texttt{up}, \texttt{down},
	   or \texttt{neutral}.
	\end{itemize}

The dataset contains approximately {1,230 training samples} and {300 test samples}, for a total of roughly {1,500 instruction examples}. It is not designed for traditional quantitative forecasting; the dataset does {not} contain
OHLCV price data, numerical time series, or regression targets. Instead, it is purely an
{NLP-oriented dataset} for {instruction tuning}. Within the FinGPT framework, the dataset is primarily used for the
{Forecaster} module, enabling LLMs to produce analyst-like reports and
directional predictions for Dow30 stocks.

There might also be some limitations for this dataset, it is not suitable for:

\begin{itemize}
	\item Numerical time-series forecasting
	\item Backtesting quantitative trading strategies
	\item Regression modeling
	\item Large-scale predictive modeling without additional data
\end{itemize}

\bibliographystyle{plain}
\bibliography{ref}
		
		
	\end{document}
